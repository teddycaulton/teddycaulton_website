# Understanding Large Language Models (LLMs) and Their Practical Applications for Engineers

In this article, I’ll refer to what is commonly called “AI” in the industry, separating Large Language Models (LLMs) and Machine Learning (ML), as understanding the difference is crucial. LLMs are a subset of ML, but the majority of ML focuses on identifying patterns in data across various domains, whereas LLMs specifically use patterns found in text data to generate text-based responses, typically word by word, to given prompts. This distinction is important for anyone in data or engineering looking to integrate LLMs into their work, as the journey is often filled with confusion and noise.

One of the biggest challenges I see when working with clients is that many are sold vague promises of LLMs accelerating their workflows without clear understanding of the practical applications. Due to the way LLMs are marketed, it’s easy for sales teams to obfuscate their true capabilities, leaving customers anxious about falling behind the technological curve. Instead of coming to the table with a specific idea of how LLMs can accelerate their work, most clients tend to approach us with a request like, “We’ve heard that AI can help speed things up—come in and show us what you can do.” This uncertainty is frustrating, especially for workers looking to get involved, as the marketing around LLMs often makes them seem more complex than they truly are, deterring engineers from exploring these tools.

The last big AI hype cycle was centered around ML, specifically classification models. While those models were highly valuable, they didn’t capture the public’s imagination in the same way, as demos often showcased metrics like accuracy and precision, which were less glamorous but extremely useful for businesses. In contrast, the emergence of ChatGPT has stirred significant excitement due to its ability to generate conversation and provide reasonably coherent outputs. The barrier to entry is low—simply visit chatgpt.com and you can begin experimenting—but this simplicity can be misleading, creating unrealistic expectations for both customers and engineers.

This presents a disservice to both groups. For customers, it sets the stage for unrealistic expectations about the potential capabilities of LLMs, while for engineers, it complicates practical applications and leads to a perception that the barrier to entry is higher than it actually is. I’ve personally tutored engineers who were surprised at how accessible working with LLMs is, especially after exposure to the marketing materials circulating on LinkedIn or from executives. Meanwhile, big tech companies like Nvidia, Microsoft, and Meta continue to pitch the idea of LLMs as the next frontier of technology, with visions of entire organizations being run by AI agents, performing jobs traditionally done by humans.

However, developers working with LLMs quickly realize that the models as they stand today are far from being able to replace entire workforces. The grand promises made by these companies muddy the waters and overshadow where LLMs truly shine: automating repetitive tasks using fewer lines of code with more dynamic outputs. It’s possible that advancements in computing and model scaling will change this in the future, but for now, LLMs are better suited for specific tasks rather than wholesale transformations of industries.

The point of this article is not to criticize the way AI is marketed to prop up overvalued tech companies but to provide engineers with a more grounded, accessible understanding of how to approach integrating LLMs into their everyday work. Regardless of how the technology evolves, I want to shed light on how data engineers can start using LLMs effectively, with an example of how I’ve utilized them to streamline repetitive tasks without resorting to complex automation.

## The Basics of LLMs and How They Work

If you're looking for a deeper dive into how LLMs like ChatGPT are trained, I recommend reading this [Medium post](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f) that provides an excellent overview with visuals. The key takeaway from that article is that these models are trained on massive amounts of data, which enables them to generate seemingly intelligent responses. However, this vast amount of data and noise is also the root cause of many of the “hallucinations” these models can experience. As a result, LLMs require highly specific and detailed prompts to produce reliable outputs.

When it comes to generating code, for instance, simply asking an LLM to "write some code" is rarely sufficient. You’ll often need to specify the programming language, what exactly you want the code to accomplish, and provide an example to guide the model. Even then, the model might only offer one potential solution, which may or may not be correct. It can struggle to deviate from that answer or provide variations when necessary.

This is where LLMs really excel for data engineers: they can take an example of how to solve a problem and then automate the generation of similar solutions. For example, if you need to write code to perform a repetitive task, an LLM can quickly help you generate that code, freeing you from manually repeating the same work.

However, using LLMs for more complex, creative tasks or unexpected situations is a different story. To use an LLM as the kind of AI agent envisioned by Nvidia would require feeding it a vast amount of structured data and hoping that it doesn't encounter any novel scenarios that require new solutions. Even then, these models have limitations on how much data they can track before their performance starts to degrade, resulting in hallucinated answers.

Despite these limitations, the practical applications of LLMs are still incredibly valuable. They offer a massive efficiency boost for engineers working on repetitive tasks. Yet, they are far from a "software engineer in a box" or a magic replacement for human work. They are tools, much like spellcheck or Excel autofill, designed to enhance our productivity.

## Practical Example: Using LLMs for a Data Migration Project

To demonstrate how LLMs can be used to accelerate work, let me walk you through a real-world example of how I used them in a data migration project. We were working with a client who needed to move their data off of an on-premise SQL Server, which was operating on a medallion architecture. The goal was to migrate the data to Snowflake, with DBT handling all ETL processes from the raw layer.

The challenge was that we had around 500 models to rebuild in DBT, all of which had been previously implemented using SQL Server stored procedures. This task would typically take two engineers around four months to complete, considering the time required to read and interpret SQL Server code, map data types to Snowflake formats, rewrite logic into DBT, and perform extensive QA. Even with maximum productivity, this would be a tight timeline.

However, I realized that a large portion of the models we needed to rebuild followed relatively simple patterns, such as truncate-and-reload or incremental models. These models had similar structures and, while not templatable in a traditional coding sense, could be understood by an LLM. The LLM could translate SQL Server code into DBT much more efficiently than a manual process. I also realized that the LLM could help with the crucial task of mapping data types between SQL Server and Snowflake, ensuring that the data migrated accurately.

To test this, I wrote a model that took an example SQL Server file and converted it into DBT code. The result was impressive—within hours, the LLM had converted a bulk load model flawlessly, handling the data type translations with accuracy. I then automated the process by writing a Python script that would grab SQL Server models, feed them to the LLM, and convert them into DBT code. The LLM achieved near-perfect accuracy on the first attempt, with only a few models requiring manual adjustments due to minor hallucinations.

By leveraging the LLM, I was able to complete 50% of the data migration work in just a few hours—work that would have traditionally taken days. This saved the engineering team significant time and effort, reducing repetitive work and allowing us to focus on more complex, non-repetitive tasks.

## Conclusion: LLMs as a Tool, Not a Replacement

The key takeaway from this example is that LLMs are powerful tools for accelerating repetitive tasks, but they are not a replacement for engineers. For the more complex tasks in the data migration process—like ETL operations that require intricate logic—LLMs were not useful. Instead, they were an excellent fit for automating the repetitive, boilerplate tasks that would have taken significant time to handle manually.

As engineers, the real value in using LLMs comes from understanding where they shine: simple, repetitive tasks that can be easily templated and automated. Trying to force them into complex, high-level tasks where they can only sometimes provide usable solutions is both inefficient and counterproductive. The future of LLMs may eventually lead to more sophisticated applications, but for now, they are best used as a tool to boost efficiency rather than as an all-encompassing solution.

Businesses and engineers alike should focus on finding these niche use cases for LLMs, where their capabilities can truly make a difference. In doing so, engineers can use these tools to be more productive, while avoiding the pitfalls of overhyped, unrealistic expectations that have clouded the broader conversation about AI.
